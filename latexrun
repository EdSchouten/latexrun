#!/usr/bin/env python3

import sys
import os
import argparse
import shlex
import json
import subprocess
import re
import collections
import hashlib

def debug(string, *args):
    if debug.enabled:
        print(string.format(*args), file=sys.stderr)
debug.enabled = False

def main():
    # Parse command-line
    arg_parser = argparse.ArgumentParser(
        description='Run LaTeX/BibTeX the right number of times')
    arg_parser.add_argument(
        '--latex-cmd', metavar='CMD', default='pdflatex',
        help='Latex command (default: %(default)s)')
    arg_parser.add_argument(
        '--latex-args', metavar='ARGS', type=arg_parser_shlex,
        help='Additional command-line arguments for latex.'
        ' This will be parsed and split using POSIX shell rules.')
    arg_parser.add_argument(
        '--bibtex-cmd', metavar='CMD', default='bibtex',
        help='Bibtex command (default: %(default)s)')
    arg_parser.add_argument(
        '--bibtex-args', metavar='ARGS', type=arg_parser_shlex,
        help='Additional command-line arguments for bibtex')
    arg_parser.add_argument(
        '--max-iterations', metavar='N', type=int, default=10,
        help='Max number of times to run latex before giving up'
        ' (default: %(default)s)')
    arg_parser.add_argument(
        '--debug', action='store_true',
        help='Enable detailed debug output')
    actions = arg_parser.add_argument_group('actions')
    actions.add_argument(
        '--clean', action='store_true', help='Delete output files')
    actions.add_argument(
        'file', nargs='?', help='Compile TeX file')
    args = arg_parser.parse_args()
    if not any([args.clean, args.file]):
        arg_parser.error('at least one action is required')
    args.latex_args = args.latex_args or []
    args.bibtex_args = args.bibtex_args or []

    debug.enabled = args.debug

    # Open control database
    # XXX Is there a better place to put this?  If I'm cleaning, I
    # don't have a file name, so it can't be relative to that.
    dbpath = '.latexrun'
    try:
        db = DB(dbpath)
    except (ValueError, OSError) as e:
        print('error opening {}: {}'.format(dbpath, e),
              file=sys.stderr)
        sys.exit(1)

    # Clean
    if args.clean:
        try:
            db.do_clean()
        except OSError as e:
            print(e, file=sys.stderr)
            sys.exit(1)

    # Build
    if not args.file:
        return
    try:
        tasks = [LaTeX(db, args.file, args.latex_cmd, args.latex_args),
#                 BibTeX(db, XXX)
        ]
        stable = run_tasks(tasks, args.max_iterations)

        # Print final task output and gather exit status
        status = 0
        for task in tasks:
            status = max(task.report(), status)

        if not stable:
            print('error: files are still changing after {} iterations; giving up'
                  .format(args.max_iterations), file=sys.stderr)
            status = max(status, 1)
    except TaskError as e:
        print(str(e), file=sys.stderr)
        status = 1
    sys.exit(status)

class TaskError(Exception):
    pass

def arg_parser_shlex(string):
    """Argument parser for shell token lists."""
    try:
        return shlex.split(string)
    except ValueError as e:
        raise argparse.ArgumentTypeError(str(e)) from None

def run_tasks(tasks, max_iterations):
    """Execute tasks in round-robin order until all are stable.

    Return True if fixed-point is reached within max_iterations
    iterations.
    """

    nstable = 0
    for iteration in range(max_iterations):
        for task in tasks:
            if task.stable():
                nstable += 1
                if nstable == len(tasks):
                    debug('fixed-point reached')
                    return True
            else:
                task.run()
                nstable = 0
    debug('fixed-point not reached')
    return False

class DB:
    """A latexrun control database."""

    _VERSION = 'latexrun-db-v1'

    def __init__(self, filename):
        self.__filename = filename

        try:
            fp = open(filename, 'r')
        except FileNotFoundError:
            debug('creating new database')
            self.__val = {'version': DB._VERSION}
        else:
            debug('loading database')
            self.__val = json.load(fp)
            if 'version' not in self.__val:
                raise ValueError('file exists, but does not appear to be a latexrun database'.format(filename))
            if self.__val['version'] != DB._VERSION:
                raise ValueError('unknown database version {!r}'
                                 .format(self.__val['version']))

    def commit(self):
        debug('committing database')
        tmp_filename = self.__filename + '.tmp'
        with open(tmp_filename, 'w') as fp:
            json.dump(self.__val, fp, indent=2, separators=(',', ': '))
            fp.flush()
            os.fsync(fp.fileno())
        os.rename(tmp_filename, self.__filename)

    def get_summary(self, task_id):
        """Return the recorded summary for the given task."""
        return self.__val.get('tasks', {}).get(task_id)

    def set_summary(self, task_id, summary):
        """Set the summary for the given task."""
        self.__val.setdefault('tasks', {})[task_id] = summary

    def add_clean(self, filename):
        """Add an output file to be cleaned.

        Unlike the output files recorded in the task summaries,
        cleanable files strictly accumulate until a clean is
        performed.
        """
        self.__val.setdefault('clean', {})[filename] = hash_cache.get(filename)

    def do_clean(self):
        """Remove output files and delete database."""

        for f, want_hash in self.__val.get('clean', {}).items():
            have_hash = hash_cache.get(f)
            if have_hash is not None:
                if want_hash == have_hash:
                    debug('unlinking {}', f)
                    hash_cache.invalidate(f)
                    os.unlink(f)
                else:
                    print('warning: {} has changed; not removing'.format(f),
                          file=sys.stderr)
        self.__val = {'version': DB._VERSION}
        try:
            os.unlink(self.__filename)
        except FileNotFoundError:
            pass

class HashCache:
    """Cache of file hashes.

    As latexrun reaches fixed-point, it hashes the same files over and
    over, many of which never change.  Since hashing is somewhat
    expensive, we keep a simple cache of these hashes.
    """

    def __init__(self):
        self.__cache = {}

    def get(self, filename):
        try:
            with open(filename, 'rb') as fp:
                st = os.fstat(fp.fileno())
                key = (st.st_dev, st.st_ino)
                if key in self.__cache:
                    return self.__cache[key]

                debug('hashing {}', filename)
                h = hashlib.sha256()
                while True:
                    block = fp.read(256*1024)
                    if not len(block):
                        break
                    h.update(block)
                self.__cache[key] = h.hexdigest()
                return self.__cache[key]
        except FileNotFoundError:
            return None

    def invalidate(self, filename):
        try:
            st = os.stat(filename)
        except OSError as e:
            # Pessimistically wipe the whole cache
            debug('wiping hash cache ({})', e)
            self.__cache.clear()
        else:
            key = (st.st_dev, st.st_ino)
            if key in self.__cache:
                del self.__cache[key]
hash_cache = HashCache()

class Task:
    """A deterministic computation whose inputs and outputs can be captured."""

    def __init__(self, db, task_id):
        self.__db = db
        self.__task_id = task_id

    def __debug(self, string, *args):
        if debug.enabled:
            debug('task {}: {}', self.__task_id, string.format(*args))

    def stable(self):
        """Return True if running this task will not affect system state.

        Functionally, let f be the task, and s be the system state.
        Then s' = f(s).  If it must be that s' == s (that is, f has
        reached a fixed point), then this function must return True.
        """
        last_summary = self.__db.get_summary(self.__task_id)
        if last_summary is None:
            # Task has never run, so running it will modify system
            # state
            changed = 'never run'
        else:
            # If any of the inputs have changed since the last run of
            # this task, the result may change, so re-run the task.
            # Also, it's possible something else changed an output
            # file, in which case we also want to re-run the task, so
            # check the outputs, too.
            changed = self.__summary_changed(last_summary)

        if changed:
            self.__debug('unstable (changed: {})', changed)
            return False
        else:
            self.__debug('stable')
            return True

    def run(self):
        # Before we run the task, hash any files that were *both* an
        # input and an output during the last run.  It's likely that
        # they'll be overwritten again by this run, and if they are,
        # it'll be too late get input hashes for these files.
        last_summary = self.__db.get_summary(self.__task_id)
        prehashes = {}
        if last_summary is not None:
            for io_filename in set(last_summary['input_files']).\
                intersection(last_summary['output_files']):
                self.__debug('pre-hashing {}', io_filename)
                prehashes[io_filename] = hash_cache.get(io_filename)

        # Run the task
        self.__debug('running')
        result = self._execute()

        # Clear cached output file hashes
        for filename in result.output_filenames:
            hash_cache.invalidate(filename)

        # Update task summary in database
        summary = self.__get_summary(result, prehashes)
        self.__db.set_summary(self.__task_id, summary)

        # Add output files to be cleaned
        for f in result.output_filenames:
            self.__db.add_clean(f)

        self.__db.commit()

    def get_config(self):
        """Return the configuration of this task.

        This is treated as an additional input, so if it differs from
        the previous run of this task, the task will be re-run.  This
        can be any JSON-able type.  The default implementation returns
        None.
        """
        return None

    def __get_summary(self, task_result, prehashes):
        """Return a task summary.

        This combines the task configuration, and all inputs and
        outputs.
        """
        input_files = {}
        for f in task_result.input_filenames:
            if f in prehashes:
                # We speculatively hashed this file before running the
                # task
                input_files[f] = prehashes[f]
            elif f in task_result.output_filenames:
                # This was both an input and an output file and now
                # it's too late to get an input hash.  Use a dummy to
                # force a re-run (we'll prehash it then)
                self.__debug('input overwritten: {}', f)
                input_files[f] = '*'
            else:
                input_files[f] = hash_cache.get(f)

        summary = {
            'config': self.get_config(),
            'env': {v: os.environ.get(v) for v in task_result.env_vars},
            'input_files': input_files,
            'output_files': {f: hash_cache.get(f)
                             for f in task_result.output_filenames},
            'extra': task_result.extra,
        }
        return summary

    def __summary_changed(self, summary):
        """Test if any inputs or outputs have changed from summary.

        Returns a string describing the changed input, or None.
        """
        # Check in order from easy to hard
        if self.get_config() != summary['config']:
            return 'configuration'
        for k, v in summary['env'].items():
            if os.environ.get(k) != v:
                return k
        for k, v in summary['input_files'].items():
            if hash_cache.get(k) != v:
                return k
        for k, v in summary['output_files'].items():
            if hash_cache.get(k) != v:
                return k
        return None

    def _execute(self):
        """Abstract: Execute this task.

        Subclasses should implement this method to execute this task.
        This method must return a TaskResult giving the inputs that
        were used by the task and the outputs it produced.
        """
        raise NotImplementedError('Task._execute is abstract')

    def report(self):
        """Report the task's results to stderr and return exit status."""
        summary = self.__db.get_summary(self.__task_id)
        if summary is None:
            return 0
        return self._report(summary['extra'])

    def _report(self, extra):
        """Abstract: Task-specific implementation of report.

        If this task has run in the past (including in previous
        invocations of latexrun), this will be called with the value
        of TaskResult.extra returned by the last invocation of execute.
        """
        return 0

class TaskResult(collections.namedtuple(
        'TaskResult', 'input_filenames output_filenames env_vars extra')):
    pass

class LaTeX(Task):
    def __init__(self, db, tex_filename, cmd, cmd_args):
        super().__init__(db, 'latex::' +
                         os.path.relpath(os.path.realpath(tex_filename)))
        self.__tex_filename = tex_filename
        self.__cmd = cmd
        self.__cmd_args = cmd_args

    def __get_args(self):
        # If filename starts with a character the tex command-line
        # treats specially, then tweak it so it doesn't.
        filename = self.__tex_filename
        if filename.startswith(('-', '&', '\\')):
            filename = './' + filename
        return [self.__cmd] + self.__cmd_args + \
            ['-interaction', 'nonstopmode', '-recorder', filename]

    def get_config(self):
        return {'args': self.__get_args()}

    def _execute(self):
        # Run latex
        # XXX Parse output
        args = self.__get_args()
        debug('running', args)
        try:
            p = subprocess.Popen(args,
                                 stdin=subprocess.DEVNULL,
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.STDOUT)
            stdout = self.__feed_terminal(p.stdout)
            # XXX Do something with the status.  If there's a fatal
            # error, should we stop immediately?  Are errors stable?
            status = p.wait()
        except OSError as e:
            raise TaskError('failed to execute latex task: ' + str(e)) from e

        jobname = self.__get_jobname(stdout)
        inputs, outputs = self.__parse_recorder(jobname)
        env_vars = ['TEXMFOUTPUT', 'TEXINPUTS', 'TEXFORMATS', 'TEXPOOL',
                    'TFMFONTS', 'PATH']
        return TaskResult(inputs, outputs, env_vars,
                          {'jobname': jobname, 'status': status})

    def __feed_terminal(self, stdout):
        buf = []
        filt = LaTeXFilter()
        while True:
            # Use os.read to read only what's available on the pipe,
            # without waiting to fill a buffer
            data = os.read(stdout.fileno(), 4096)
            if not data:
                break
            buf.append(data)
            filt.feed(data)
            current_file = filt.get_current_file()
            if current_file:
                # XXX Generalize progress reporting
                # XXX Deduplicate messages
                sys.stderr.write('\r\x1b[K\x1b[?7l' + '[latex] ' +
                                 current_file[:79] +
                                 '\x1b[?7h')
                sys.stderr.flush()
        sys.stderr.write('\r\x1b[K')
        sys.stderr.flush()
        return b''.join(buf)

    def __get_jobname(self, stdout):
        """Extract the job name from latex's output.

        We get this from latex because it depends on complicated file
        name parsing rules, is affected by arguments like
        -output-directory, and may be just "texput" if things fail
        really early.
        """
        jobname = None
        for m in re.finditer(br'^Transcript written on (.*)\.log\.$', stdout,
                             re.MULTILINE):
            jobname = m.group(1).decode()
        if jobname is None:
            print(stdout.decode(), file=sys.stderr)
            raise TaskError('failed to extract job name from latex log')
        return jobname

    def __parse_recorder(self, jobname):
        """Parse file recorder output."""
        # XXX If latex fails because a file isn't found, that doesn't
        # go into the .fls file, but creating that file will affect
        # the computation, so it should be included as an input.
        # Though it's generally true that files can be added earlier
        # in search paths and will affect the output without us knowing.
        filename = jobname + '.fls'
        try:
            recorder = open(filename)
        except OSError as e:
            raise TaskError('failed to open file recorder output: ' + str(e)) \
                from e
        pwd, inputs, outputs = '', set(), set()
        for linenum, line in enumerate(recorder):
            parts = line.rstrip('\n').split(' ', 1)
            if parts[0] == 'PWD':
                pwd = parts[1]
            elif parts[0] == 'INPUT':
                inputs.add(os.path.join(pwd, parts[1]))
            elif parts[0] == 'OUTPUT':
                outputs.add(os.path.join(pwd, parts[1]))
            else:
                raise TaskError('syntax error on line {} of {}'
                                .format(linenum, filename))
        # Ironically, latex omits the .fls file itself
        outputs.add(filename)
        return inputs, outputs

    def _report(self, extra):
        # Parse the log
        logfile = open(extra['jobname'] + '.log', 'rb')
        for msg in LaTeXFilter().feed(logfile.read(), True).get_messages():
            print(msg, file=sys.stderr)

        # Return LaTeX's exit status
        return extra['status']

class LaTeXFilter:
    def __init__(self):
        self.__data = b''
        self.__restart_pos = 0
        self.__restart_file_stack = []
        self.__restart_messages_len = 0
        self.__messages = []

    def feed(self, data, eof=False):
        self.__data += data
        self.__data_complete = eof

        # Reset to last known-good restart point
        self.__pos = self.__restart_pos
        self.__file_stack = self.__restart_file_stack.copy()
        self.__messages = self.__messages[:self.__restart_messages_len]
        self.__lstart = self.__lend = -1

        # Parse forward
        while self.__pos < len(self.__data):
            self.__noise()

        if eof and len(self.__file_stack):
            self.__message('warning', None,
                           "unbalanced `(' in log; file names may be wrong")
        return self

    def get_messages(self):
        return self.__messages

    def get_current_file(self):
        if self.__file_stack:
            return self.__file_stack[-1]
        return None

    def __save_restart_point(self):
        self.__restart_pos = self.__pos
        self.__restart_file_stack = self.__file_stack.copy()
        self.__restart_messages_len = len(self.__messages)

    def __message(self, typ, lineno, msg):
        pos = self.__file_stack[-1] if self.__file_stack else '<no file>'
        if pos.startswith('./'):
            pos = pos[2:]
        if lineno is not None:
            pos += ':' + str(lineno)

        self.__messages.append('{}: {}: {}'.format(pos, typ, msg))

    def __ensure_line(self):
        if self.__lstart <= self.__pos < self.__lend:
            return
        self.__lstart = self.__data.rfind(b'\n', 0, self.__pos) + 1
        self.__lend = self.__data.find(b'\n', self.__pos) + 1
        if self.__lend == 0:
            self.__lend = len(self.__data)

    @property
    def __col(self):
        self.__ensure_line()
        return self.__pos - self.__lstart

    @property
    def __avail(self):
        return self.__pos < len(self.__data)

    def __lookingat(self, needle):
        return self.__data.startswith(needle, self.__pos)

    def __lookingatre(self, regexp, flags=0):
        return re.compile(regexp, flags=flags).match(self.__data, self.__pos)

    def __skip_line(self):
        self.__ensure_line()
        self.__pos = self.__lend

    def __consume_line(self, unwrap=False):
        self.__ensure_line()
        data = self.__data[self.__pos:self.__lend]
        self.__pos = self.__lend
        if unwrap:
            # TeX helpfully wraps all terminal output at 79 columns
            # (max_print_line).  If requested, unwrap it.  There's
            # simply no way to do this perfectly, since there could be
            # a line that happens to be 79 columns.
            while self.__lend - self.__lstart == 80:
                self.__ensure_line()
                data = data[:-1] + self.__data[self.__pos:self.__lend]
                self.__pos = self.__lend
        return data

    # Parser productions

    def __noise(self):
        # Most of TeX's output is line noise that combines error
        # messages, warnings, file names, user errors and warnings,
        # and echos of token lists and other input.  This attempts to
        # tease these apart, paying particular attention to all of the
        # places where TeX echos input so that parens in the input do
        # not confuse the file name scanner.  There are three
        # functions in TeX that echo input: show_token_list (used by
        # runaway and show_context, which is used by print_err),
        # short_display (used by overfull/etc h/vbox), and show_print
        # (used in issue_message and the same places as
        # show_token_list).
        lookingat, lookingatre = self.__lookingat, self.__lookingatre
        if self.__col == 0:
            # The following messages are always preceded by a newline
            if lookingat(b'! '):
                return self.__errmessage()
            if lookingat(b'!pdfTeX error: '):
                return self.__pdftex_fail()
            if lookingat(b'Runaway '):
                return self.__runaway()
            if lookingatre(br'(Overfull|Underfull|Loose|Tight) \\[hv]box \('):
                return self.__bad_box()
            if lookingatre(b'(Package |Class |LaTeX |pdfTeX )?(\w+ )?warning: ', re.I):
                return self.__generic_warning()
            # Other things that are common and irrelevant
            if lookingatre(br'(Package|Class|LaTeX) (\w+ )?info: ', re.I):
                return self.__generic_info()
            if lookingatre(br'(Document Class|File|Package): '):
                # Output from "\ProvidesX"
                return self.__consume_line(unwrap=True)
            if lookingatre(br'\\\w+=\\[a-z]+\d+\n'):
                # Output from "\new{count,dimen,skip,...}"
                return self.__consume_line(unwrap=True)

        # print(self.__data[self.__lstart:self.__lend].decode().rstrip())
        # self.__pos = self.__lend
        # return

        # Now that we've substantially reduced the spew and hopefully
        # eliminated all input echoing, we're left with the file name
        # stack, page outs, and random other messages from both TeX
        # and various packages.  We'll assume at this point that all
        # parentheses belong to the file name stack or, if they're in
        # random other messages, they're at least balanced and nothing
        # interesting happens between them.
        m = re.compile(b'[(){}\n]').search(self.__data, self.__pos)
        if m is None:
            self.__pos = len(self.__data)
            return
        self.__pos = m.start() + 1
        ch = self.__data[m.start()]
        if ch == ord('\n'):
            # Save this as a known-good restart point for incremental
            # parsing, since we definitely didn't match any of the
            # known message types above.
            self.__save_restart_point()
        elif ch == ord('('):
            # XXX Check that the stack doesn't drop to empty and then re-grow
            self.__file_stack.append(self.__filename())
        elif ch == ord(')'):
            if len(self.__file_stack):
                self.__file_stack.pop()
            else:
                self.__message('warning', None,
                               "extra `)' in log; file names may be wrong ")
        elif ch == ord('{'):
            # TeX uses this for various things we want to ignore, like
            # file names and print_mark.  Consume up to the '}'
            epos = self.__data.find(b'}', self.__pos)
            if epos != -1:
                self.__pos = epos + 1
            else:
                self.__message('warning', None,
                               "unbalanced `{' in log; file names may be wrong")
        elif ch == ord('}'):
            self.__message('warning', None,
                           "extra `}' in log; file names may be wrong")

    def __filename(self):
        initcol = self.__col
        first = True
        name = ''
        # File names may wrap, but if they do, TeX will always print a
        # newline before the open paren
        while first or (initcol == 1 and self.__col == 79):
            if not first:
                self.__pos += 1
            m = self.__lookingatre(br'[^(){} \n]*')
            name += m.group().decode()
            self.__pos = m.end()
            first = False
        return name

    def __errmessage(self):
        # Procedure print_err (including \errmessage, itself used by
        # LaTeX's \GenericError and all of its callers).  Prints "\n!
        # " followed by error text.  print_err is always followed by a
        # call to error, which prints a period, and a newline...
        msg = self.__consume_line(unwrap=True)[1:].strip()
        msg = re.sub(b'^LaTeX Error: ', b'', msg)
        msg = re.sub(br'\.$', b'', msg)
        # ... and then calls show_context, which prints the input
        # stack as pairs of lines giving the context.  These context
        # lines are truncated so they never wrap.  Each pair of lines
        # will start with either "<something> " if the context is a
        # token list, "<*> " for terminal input (or command line),
        # "<read ...>" for stream reads, or "l.[0-9]+ " if it's a
        # file.  This is followed by the errant input with a line
        # break where the error occurred.
        lineno = None
        found_context = False
        while self.__avail:
            if self.__lookingatre(br'<([a-z]+|\*|read [^ >]*)> '):
                found_context = True
                self.__skip_line()
            elif self.__lookingatre(b'l.[0-9]+ '):
                found_context = True
                info = self.__consume_line().split(b' ', 1)
                lineno = int(info[0][2:])
            elif found_context:
                # Done with context
                break
            # If we haven't found the context, skip the line.  If we
            # have found the context, skip the second context line.
            self.__skip_line()
        self.__message('error', lineno, msg.decode())

    def __pdftex_fail(self):
        # Procedure pdftex_fail.  Prints "\n!pdfTeX error: ", the
        # message, and a newline.  Unlike print_err, there's never
        # context.
        msg = self.__consume_line(unwrap=True)[1:].strip()
        self.__message('error', None, msg.decode())

    def __runaway(self):
        # Procedure runaway.  Prints "\nRunaway ...\n" followed by
        # token list (user text).  Always followed by a call to
        # print_err, so skip lines until we see the print_err.
        self.__skip_line()      # Skip "Runaway ...\n"
        self.__skip_line()      # Skip token list
        while not self.__lookingat(b'!') and self.__avail:
            self.__skip_line()

    def __bad_box(self):
        # Function hpack and vpack.  hpack prints a warning, a
        # newline, then a short_display of the offending text.
        # Unfortunately, there's nothing indicating the end of the
        # offending text, but it should be on one (possible wrapped)
        # line.  vpack prints a warning and then, *unless output is
        # active*, a newline.  The missing newline is probably a bug,
        # but it sure makes our lives harder.
        origpos = self.__pos
        msg = self.__consume_line()
        m = re.search(br' in (?:paragraph|alignment) at lines ([0-9]+)--([0-9]+)', msg) or \
            re.search(br' detected at line ([0-9]+)', msg)
        if m:
            lineno = int(m.group(1))
            msg = msg[:m.start()]
        else:
            m = re.search(br' while \\output is active', msg)
            if m:
                lineno = None
                msg = msg[:m.end()]
            else:
                self.__message('warning', None,
                               'malformed bad box message in log')
                return
        # Back up to the end of the known message text
        self.__pos = origpos + m.end()
        if self.__lookingat(b'\n'):
            # We have a newline, so consume it and look for the
            # offending text.
            self.__pos += 1
            # If there is offending text, it will start with a font
            # name, which will start with a \.
            if b'hbox' in msg and self.__lookingat(b'\\'):
                self.__consume_line(unwrap=True)
        self.__message('warning', lineno, msg.decode())

    def __generic_warning(self):
        # Warnings produced by LaTeX's \GenericWarning (which is
        # called by \{Package,Class}Warning and \@latex@warning),
        # warnings produced by pdftex_warn, and other random warnings.
        msg = self.__generic_info()
        msg = re.sub(b'^LaTeX Warning: ', b'', msg)
        msg = re.sub(b' ?warning:', b':', msg, flags=re.I)
        msg = re.sub(br'\.$', b'', msg)
        # Most warnings include an input line emitted by \on@line
        m = re.match(b' on input line ([0-9]+)', msg)
        if m:
            lineno = int(m.group(1))
            msg = msg[m.start()]
        else:
            lineno = None
        self.__message('warning', lineno, msg.decode())

    def __generic_info(self):
        # Messages produced by LaTeX's \Generic{Error,Warning,Info}
        # and things that look like them
        msg = self.__consume_line(unwrap=True).strip()
        # Package and class messages are continued with lines
        # containing '(package name)            '
        pkg_name = msg.split(b' ', 2)[1]
        prefix = b'(' + pkg_name + b')            '
        while self.__lookingat(prefix):
            self.__consume_line(unwrap=True)
        return msg

class BibTeX(Task):
    def stable(self, db):
        # XXX First check if bibtex even has its input files

        # XXX If bibtex's inputs aren't ready, then bibtex will simply
        # fail without affecting system state, so this task is
        # trivially stable.

        # XXX We don't know where the .aux file is until we've run
        # LaTeX.
        if not XXX_have_inputs:
            return True
        return super().stable(db)

    def _execute(self):
        # XXX BibTeX doesn't depend on the *whole* .aux file.  In
        # fact, we can extract exactly what it depends on.  Maybe we
        # shouldn't record the .aux as an input, but instead record
        # the important parts in our task configuration?
        pass

class Kpathsea:
    def __init__(self, program_name):
        self.__progname = program_name

    def find_file(self, name, format):
        """Return the resolved path of 'name' or None."""

        args = ['kpsewhich', '-progname', self.__progname, '-format', format,
                name]
        try:
            return subprocess.check_output(args, universal_newlines=True).strip()
        except subprocess.CalledProcessError as e:
            if e.returncode != 1:
                raise
            return None

if __name__ == "__main__":
    main()
